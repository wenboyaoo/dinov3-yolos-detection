lr: 8.0e-4
backbone_lr: 8.0e-5
batch_size: 32
weight_decay: 5.0e-4
epochs: 75
clip_max_norm: 0.1
bf16: True
eval_during_training: True
eval_epochs: 1
save_epochs: 15

# scheduler
sched: warmupcos # options: ["step", "warmupcos"]
## warmupcosine
warmup_lr: 1.0e-6
min_lr: 1.0e-6
warmup_epochs: 5
decay_rate: 0.1

# model setting
num_det_tokens: 64
backbone: dinov3 # options: ["vanilla_vit", "dinov3"]
pretrained: facebook/dinov3-vits16-pretrain-lvd1689m
unfreeze: 
  - class_embed
  - bbox_embed
  - backbone.embeddings.det_tokens
  - backbone.det_additive_embeddings
  - backbone.det_rope_params
  - backbone.det_rope_updater
no_weight_decay:
  - det_tokens
  - det_additive_embeddings
  - det_rope_params
## vanilla_vit
backbone_size: small
use_checkpoint: False
init_pe_size: [512,864]
mid_pe_size: [512,864]

# Matcher
set_cost_class: 4
set_cost_bbox: 5
set_cost_giou: 2
# Loss coefficients
ce_loss_coef: 4
bbox_loss_coef: 5
giou_loss_coef: 2
eos_coef: 0.2

# dataset parameters
dataset_file: voc # options: ["coco","voc"]
num_classes: 20
coco_path: data/VOC2012_train_val
remove_difficult: False
output_dir: outputs/sample
device: cuda
resize: 512

seed: 42
resume: # resume from checkpoint
eval: False
start_epoch: 0
num_workers: 64

# experiments
## attention gates
det_token_gate: # ['block_non_det_read_det', 'block_all_read_det']
## backup matching
backup_matching: False
backup_class_ids: []
backup_k_scale: 0.2
backup_top_k: 5
## det postion embedding
### additive
enable_det_additive_embeddings: False
### RoPE
enable_det_rope: False
det_rope_type: fixed # ['fixed', 'learnable', 'adaptive']
#### adaptive RoPE
token_proj_dim: 32
attn_interp_dim: 50
attn_proj_dim: 32
det_mlp_intermediate_size: 64
max_stride: 0.2
det_tem_power_scale: 2.0
aux_loss_coef: 1.0
rope_center_loss_coef: 0.1

# >>> DEBUG COST RATIO LOGGING (TEMP; safe to delete) >>>
# Print per-class top-k unmatched cost / cost_min ratios each batch (k=1,5,10).
debug_cost_ratio: False
# <<< DEBUG COST RATIO LOGGING (TEMP; safe to delete) <<<